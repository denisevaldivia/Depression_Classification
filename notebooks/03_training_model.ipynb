{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5230de6c",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 15px 15px 15px 15px;\" src=\"https://img.freepik.com/free-vector/depression-concept-illustration_114360-3747.jpg?t=st=1657678284~exp=1657678884~hmac=b8b1d71ca0a8eb2e4ff5bf31d6a98624112f1a2254b0f39e92254ed12d7875b2\" width=\"240px\" height=\"180px\" />\n",
    "\n",
    "# <font color= #bbc28d> **Clasificación de Depresión - Modelado** </font>\n",
    "#### <font color= #2E9AFE> `Proyecto de Ciencia de Datos`</font>\n",
    "- <Strong> Sofía Maldonado, Diana Valdivia, Samantha Sánchez & Vivienne Toledo </Strong>\n",
    "- <Strong> Fecha </Strong>: 11/11/2025.\n",
    "\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Image retrieved from: https://img.freepik.com/free-vector/depression-concept-illustration_114360-3747.jpg?t=st=1657678284~exp=1657678884~hmac=b8b1d71ca0a8eb2e4ff5bf31d6a98624112f1a2254b0f39e92254ed12d7875b2/p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bbc203",
   "metadata": {},
   "source": [
    "# <font color= #bbc28d>**Datos y Data Readiness** </font>\n",
    "En esta etapa se siguió trabajando con la misma base de datos la cuál intenta clasificar a los jóvenes estudiantes en dos categorías: aquellos que presentan síntomas de depresión y aquellos que no, siendo esta columna nuestro objetivo. Aplicando diferentes transformaciones y limpiezas para preparar los datos antes del modelado. Las acciones principales fueron:\n",
    "\n",
    "#### <font color=#99c0c4>1. **Tratamiento de datos faltantes** </font>\n",
    "- Se detectó que la columna Financial Stress contenía 3 valores nulos.\n",
    "- Dado que era una cantidad pequeña en comparación con el total de registros, se decidió eliminar esas filas.\n",
    "\n",
    "#### <font color=#99c0c4>2. **Filtrado de datos categóricos** </font>\n",
    "- Se identificaron variables con categorías poco representativas (con una o dos filas por valor), como City, Dietary Habits, Sleep Duration y Degree.\n",
    "- Se eliminaron registros aislados para reducir ruido y mejorar la representatividad de los datos.\n",
    "\n",
    "#### <font color=#99c0c4>3. **Ajuste del dataset al enfoque del proyecto** </font>\n",
    "- Se filtraron los registros de Age para conservar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd9446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Databricks Env\n",
    "import pathlib\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Optimization\n",
    "import math\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7b739",
   "metadata": {},
   "source": [
    "## <font color= #bbc28d>• **Credenciales & Set-up inicial** </font>\n",
    "Para poder trabajar con MLFlow es necesario ingresar con nuestros tokens de acceso y definir la base con la que estaremos trabajando, en nuestro caso será con Databricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a36e595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/11 16:25:30 INFO mlflow.tracking.fluent: Experiment with name '/Users/pipochatgpt@gmail.com/PEPE' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "ename": "RestException",
     "evalue": "RESOURCE_DOES_NOT_EXIST: Parent directory /Users/pipochatgpt@gmail.com does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m EXPERIMENT_NAME = \u001b[33m\"\u001b[39m\u001b[33m/Users/pipochatgpt@gmail.com/PEPE\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33mdatabricks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m experiment = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\tracking\\fluent.py:194\u001b[39m, in \u001b[36mset_experiment\u001b[39m\u001b[34m(experiment_name, experiment_id)\u001b[39m\n\u001b[32m    189\u001b[39m _logger.info(\n\u001b[32m    190\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExperiment with name \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist. Creating a new experiment.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    191\u001b[39m     experiment_name,\n\u001b[32m    192\u001b[39m )\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     experiment_id = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.error_code == \u001b[33m\"\u001b[39m\u001b[33mRESOURCE_ALREADY_EXISTS\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# NB: If two simultaneous processes attempt to set the same experiment\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# simultaneously, a race condition may be encountered here wherein\u001b[39;00m\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# experiment creation fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\telemetry\\track.py:24\u001b[39m, in \u001b[36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_telemetry_disabled() \u001b[38;5;129;01mor\u001b[39;00m _is_telemetry_disabled_for_event(event):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     27\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:298\u001b[39m, in \u001b[36mTrackingServiceClient.create_experiment\u001b[39m\u001b[34m(self, name, artifact_location, tags)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create an experiment.\u001b[39;00m\n\u001b[32m    285\u001b[39m \n\u001b[32m    286\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \n\u001b[32m    296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m _validate_experiment_artifact_location(artifact_location)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mExperimentTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:254\u001b[39m, in \u001b[36mRestStore.create_experiment\u001b[39m\u001b[34m(self, name, artifact_location, tags)\u001b[39m\n\u001b[32m    250\u001b[39m tag_protos = [tag.to_proto() \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags] \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    251\u001b[39m req_body = message_to_json(\n\u001b[32m    252\u001b[39m     CreateExperiment(name=name, artifact_location=artifact_location, tags=tag_protos)\n\u001b[32m    253\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCreateExperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response_proto.experiment_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\databricks_rest_store.py:100\u001b[39m, in \u001b[36mDatabricksTracingRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_endpoint\u001b[39m(\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     93\u001b[39m     api,\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     response_proto=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     98\u001b[39m ):\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m            \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    109\u001b[39m             e.error_code == ErrorCode.Name(INVALID_PARAMETER_VALUE)\n\u001b[32m    110\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCould not resolve a SQL warehouse ID\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e.message\n\u001b[32m    111\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:203\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[39m\n\u001b[32m    201\u001b[39m     endpoint, method = \u001b[38;5;28mself\u001b[39m._METHOD_TO_INFO[api]\n\u001b[32m    202\u001b[39m response_proto = response_proto \u001b[38;5;129;01mor\u001b[39;00m api.Response()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:596\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[39m\n\u001b[32m    593\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m    594\u001b[39m     response = http_request(**call_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m response_to_parse = response.text\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vivienne\\apps\\data_science_project\\Depression_Classification\\.venv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:315\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response.text):\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json.loads(response.text))\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m         base_msg = (\n\u001b[32m    318\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m         )\n",
      "\u001b[31mRestException\u001b[39m: RESOURCE_DOES_NOT_EXIST: Parent directory /Users/pipochatgpt@gmail.com does not exist."
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Load .env and Log in to Databricks\n",
    "# ======================================\n",
    "\n",
    "# Cargar las variables del archivo .env\n",
    "load_dotenv(override=True)  # Carga las variables del archivo .env\n",
    "EXPERIMENT_NAME = \"/Users/pipochatgpt@gmail.com/PEPE\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "experiment = mlflow.set_experiment(experiment_name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e98f0",
   "metadata": {},
   "source": [
    "## <font color= #bbc28d>• **Preprocesamiento** </font>\n",
    "\n",
    "Convertimos lo planteado en la libreta de limpieza de datos en una función que limpie y preprocese los datos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734d7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/depression_dataset.csv\")\n",
    "\n",
    "def clean_data(df, save_data=False):\n",
    "    # 1. Eliminar valores nulos\n",
    "    df = df.dropna()\n",
    "\n",
    "\n",
    "    # 2. Filtrado de categorías que otorgan poca información debido a su baja prevalencia\n",
    "    # City\n",
    "    ciudades = df['City'].value_counts()[df['City'].value_counts() < 450]\n",
    "    df = df[~df['City'].isin(ciudades.index)]\n",
    "    # Dietary Habits\n",
    "    df = df[df['Dietary Habits'] != 'Others']\n",
    "    # Sleep Duration\n",
    "    df = df[df['Sleep Duration'] != 'Others']\n",
    "    # Degree\n",
    "    df = df[df['Degree'] != 'Others']\n",
    "    # Age\n",
    "    df = df[df['Age'] <= 35]\n",
    "    # Academic Pressure\n",
    "    df = df[df['Academic Pressure'] > 0]\n",
    "    # Study Satisfaction\n",
    "    df = df[df['Study Satisfaction'] > 0]\n",
    "\n",
    "    # 3. Eliminar variables que no son buenas predictoras\n",
    "    df.drop(columns=['Work Pressure', 'Profession', 'Job Satisfaction', 'id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # 4. Mapear las variables categóricas binarias\n",
    "    gender = {'Male' : 0, 'Female' : 1}\n",
    "    general = {'Yes' : 1, 'No' : 0}\n",
    "    df['Gender'] = df['Gender'].map(gender)\n",
    "    df['Have you ever had suicidal thoughts ?'] = df['Have you ever had suicidal thoughts ?'].map(general)\n",
    "    df['Family History of Mental Illness'] = df['Family History of Mental Illness'].map(general)\n",
    "\n",
    "\n",
    "    # 5. Mapear las variables categóricas múltiples\n",
    "    degree = {\n",
    "    \"Class 12\": \"Secondary\",\n",
    "    \"B.Pharm\": \"Undergraduate\", \"BSc\": \"Undergraduate\", \"BA\": \"Undergraduate\", \"BCA\": \"Undergraduate\",\n",
    "    \"B.Ed\": \"Undergraduate\", \"LLB\": \"Undergraduate\", \"BE\": \"Undergraduate\", \"BHM\": \"Undergraduate\",\n",
    "    \"B.Com\": \"Undergraduate\", \"B.Arch\": \"Undergraduate\", \"B.Tech\": \"Undergraduate\", \"BBA\": \"Undergraduate\",\n",
    "    \"M.Tech\": \"Postgraduate\", \"M.Ed\": \"Postgraduate\", \"MSc\": \"Postgraduate\", \"M.Pharm\": \"Postgraduate\",\n",
    "    \"MCA\": \"Postgraduate\", \"MA\": \"Postgraduate\", \"MBA\": \"Postgraduate\", \"M.Com\": \"Postgraduate\", \"MHM\": \"Postgraduate\",\n",
    "    \"PhD\": \"Doctorate\", \"MD\": \"Doctorate\", \"MBBS\": \"Doctorate\", \"LLM\": \"Doctorate\", \"ME\": \"Postgraduate\"\n",
    "    }\n",
    "    orden_degree = {\"Secondary\": 0, \"Undergraduate\": 1, \"Postgraduate\": 2, \"Doctorate\": 3}\n",
    "    orden_alimentos = {'Healthy': 0, 'Unhealthy': 1, 'Moderate': 2}\n",
    "    orden_siesta = {'Less than 5 hours': 0, '5-6 hours': 1, '7-8 hours': 2,'More than 8 hours': 3}\n",
    "    # Aplicar el mapeo\n",
    "    df['Degree'] = df['Degree'].map(degree)\n",
    "    df['Degree'] = df['Degree'].map(orden_degree)\n",
    "    df['Dietary Habits'] = df['Dietary Habits'].map(orden_alimentos)\n",
    "    df['Sleep Duration'] = df['Sleep Duration'].map(orden_siesta)\n",
    "\n",
    "\n",
    "    # 6. Train-Test-Val Split (70-20-10)\n",
    "    X = df.drop(['Depression'], axis=1)\n",
    "    y = df['Depression']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.66, random_state=42)\n",
    "\n",
    "    if save_data:\n",
    "        # Guardar las variables \n",
    "        X_train.to_csv(r'..\\data\\interim\\X_train.csv', index=False)\n",
    "        X_test.to_csv(r'..\\data\\interim\\X_test.csv', index=False)\n",
    "        X_val.to_csv(r'..\\data\\interim\\X_val.csv', index=False)\n",
    "        \n",
    "        y_train.to_csv(r'..\\data\\processed\\y_train.csv', index=False)\n",
    "        y_test.to_csv(r'..\\data\\processed\\y_test.csv', index=False)\n",
    "        y_val.to_csv(r'..\\data\\processed\\y_val.csv', index=False)\n",
    "    \n",
    "    # Convertir las variables dependientes en NumPy arrays\n",
    "    y_train = y_train.to_numpy().ravel()\n",
    "    y_test = y_test.to_numpy().ravel()\n",
    "    y_val = y_val.to_numpy().ravel()   \n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(X_train, X_test, X_val=None, save_data=False, save_artifacts=True):\n",
    "    # Codificar variables múltiples mediante One-Hot\n",
    "    encoder = OneHotEncoder(\n",
    "        drop='first',\n",
    "        handle_unknown='ignore',        # Evita error si aparece algo nuevo\n",
    "        sparse_output=False\n",
    "    )\n",
    "\n",
    "    # Entrenar el objeto con los datos del train\n",
    "    encoder.fit(X_train[['City']])\n",
    "    \n",
    "    # Aplicar One-Hot\n",
    "    X_train_city = encoder.transform(X_train[['City']])\n",
    "    X_test_city = encoder.transform(X_test[['City']])\n",
    "    X_val_city = encoder.transform(X_val[['City']]) if X_val is not None else None\n",
    "    \n",
    "    # Obtener los nombres del One-Hot\n",
    "    city_cols = encoder.get_feature_names_out(['City'])  # Nombres automáticos de columnas\n",
    "    \n",
    "    # Crear un df con las columnas codificadas\n",
    "    X_train_city_df = pd.DataFrame(X_train_city, columns=city_cols, index=X_train.index)\n",
    "    X_test_city_df = pd.DataFrame(X_test_city, columns=city_cols, index=X_test.index)\n",
    "    X_val_city_df = pd.DataFrame(X_val_city, columns=city_cols, index=X_val.index) if X_val is not None else None\n",
    "    \n",
    "    # Eliminar la columna original en el dataset\n",
    "    X_train = X_train.drop(columns=['City'])\n",
    "    X_test = X_test.drop(columns=['City'])\n",
    "    X_val = X_val.drop(columns=['City']) if X_val is not None else None\n",
    "    \n",
    "    # Juntar las nuevas columnas con el dataset antiguo\n",
    "    X_train_final = pd.concat([X_train, X_train_city_df], axis=1)\n",
    "    X_test_final = pd.concat([X_test, X_test_city_df], axis=1)\n",
    "    X_val_final = pd.concat([X_val, X_val_city_df], axis=1) if X_val is not None else None\n",
    "\n",
    "    # Aplicar una estandarización a los datos\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "    X_test_scaled = scaler.transform(X_test_final)\n",
    "    X_val_scaled = scaler.transform(X_val_final) if X_val is not None else None\n",
    "\n",
    "    # Guardar los artefactos\n",
    "    if save_artifacts:\n",
    "        os.makedirs(\"artifacts/preprocessor\", exist_ok=True)\n",
    "\n",
    "        # Save encoder\n",
    "        with open('artifacts/preprocessor/encoder.pkl', 'wb') as f_out:\n",
    "            pickle.dump(encoder, f_out)\n",
    "        # Save scaler\n",
    "        with open('artifacts/preprocessor/scaler.pkl', 'wb') as f_out:\n",
    "            pickle.dump(scaler, f_out)\n",
    "\n",
    "        # Log artifacts to MLflow\n",
    "        mlflow.log_artifact(\"artifacts/preprocessor/encoder.pkl\", artifact_path=\"preprocessor\")\n",
    "        mlflow.log_artifact(\"artifacts/preprocessor/scaler.pkl\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        print(\"Preprocessor artifacts (encoder & scaler) successfully logged to MLflow.\")\n",
    "\n",
    "    if save_data:\n",
    "        # Regresar los datos a dataframe y guardarlos\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=X_train_final.columns, index=X_train_final.index)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=X_test_final.columns, index=X_test_final.index)\n",
    "        X_val_df = pd.DataFrame(X_val_scaled, columns=X_val_final.columns, index=X_val_final.index) if X_val is not None else None\n",
    "\n",
    "        X_train_df.to_csv(r'..\\data\\processed\\X_train.csv', index=False)\n",
    "        X_test_df.to_csv(r'..\\data\\processed\\X_test.csv', index=False)\n",
    "        X_val_df.to_csv(r'..\\data\\processed\\X_val.csv', index=False)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, X_val_scaled, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55425de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and obtain the targets\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fc299f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlflow.data.numpy_dataset.NumpyDataset at 0x1c1b9267750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subir los datos a MLflow\n",
    "mlflow.data.from_numpy(X_train, targets=y_train, name=\"depression_train\")\n",
    "mlflow.data.from_numpy(X_test, targets=y_test, name=\"depression_test\")\n",
    "mlflow.data.from_numpy(X_val, targets=y_val, name=\"depression_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b74e27",
   "metadata": {},
   "source": [
    "## <font color= #bbc28d>• **Modelado** </font>\n",
    "Retomando un poco lo de entregas pasadas, este proyecto trabaja con un conjunto de datos cuyo objetivo es  **clasificar a estudiantes** en dos categorías: aquellos que presentan **síntomas de depresión** y **aquellos que no**. Debido a la naturaleza de los datos, estamos hablando de un problema de clasificación binaria, así que para esto, elegiremos modelos que se ajustan bien a este tipo de problemas:\n",
    "- Logistic Regression\n",
    "- SVC\n",
    "- XGBoost\n",
    "\n",
    "A continuación realizaremos la **optimización de hiperparámetros** y el **entrenamiento de tres modelos de clasificación binaria**. Para cada modelo:\n",
    "\n",
    "1. Se utiliza **Optuna** para explorar diferentes combinaciones de hiperparámetros y maximizar la `F1-score` (Esta es la métrica más balanceada ya que es un promedio). Cada combinación de parámetros se evalúa mediante una función objetivo (`objective`) que entrena el modelo, realiza predicciones sobre el conjunto de prueba y calcula métricas de rendimiento como `accuracy`, `precision`, `f1` y `recall`.\n",
    "\n",
    "2. Se emplea **MLflow** para hacer un seguimiento automático de los experimentos (`autolog`) y registrar los parámetros, métricas y modelos entrenados. \n",
    "\n",
    "3. Para Logistic Regression y SVC, se crean estudios de Optuna que prueban un número definido de configuraciones (`n_trials=3`) y se seleccionan los mejores parámetros encontrados. Para XGBoost, además se ajustan hiperparámetros como número de árboles, profundidad máxima, tasa de aprendizaje y gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e134cff",
   "metadata": {},
   "source": [
    "### <font color= #7fb2b5>• **Logistic Regression** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_tuning_lr(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # Preprocess data and log artifacts\n",
    "    X_train_scaled, X_test_scaled, _, encoder, scaler = preprocessor(X_train, X_test, X_val=None, save_artifacts=True)\n",
    "\n",
    "    # Get MLflow ID to store the preprocessing artifacts\n",
    "    preprocessor_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    # Start Optuna and MLflow\n",
    "    def objective_lr(trial: optuna.trial.Trial):\n",
    "        params = {\n",
    "            'penalty': trial.suggest_categorical('penalty', ['l2','l1','elasticnet']),\n",
    "            'solver': 'saga'\n",
    "        }\n",
    "\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.set_tag('model_family', 'logistic_regression')\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param('preprocessor_run_id', preprocessor_run_id)\n",
    "\n",
    "            lr_model = LogisticRegression(**params)\n",
    "            lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Get predictions and metrics\n",
    "            y_pred = lr_model.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric('acc', acc)\n",
    "            mlflow.log_metric('precision', precision)\n",
    "            mlflow.log_metric('f1', f1)\n",
    "            mlflow.log_metric('recall', recall)\n",
    "\n",
    "            signature = infer_signature(X_test_scaled, y_pred)\n",
    "\n",
    "            # Log the trained model\n",
    "            mlflow.sklearn.log_model(\n",
    "                lr_model,\n",
    "                name='lr_model',\n",
    "                input_example=X_test_scaled[:5],\n",
    "                signature=signature\n",
    "            )\n",
    "        \n",
    "        return f1\n",
    "    \n",
    "    sampler = TPESampler(seed=42)\n",
    "    lr_study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    with mlflow.start_run(run_name='Logistic Regression (Optuna)', nested=True):\n",
    "        lr_study.optimize(objective_lr, n_trials=2)\n",
    "    \n",
    "    best_params_lr = lr_study.best_params\n",
    "\n",
    "    return best_params_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013af90",
   "metadata": {},
   "source": [
    "### <font color= #7fb2b5>• **SVC** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1754a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_tuning_svc(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # Preprocess data and log artifacts\n",
    "    X_train_scaled, X_test_scaled, _, encoder, scaler = preprocessor(X_train, X_test, X_val=None, save_artifacts=True)\n",
    "\n",
    "    # Get MLflow ID to store the preprocessing artifacts\n",
    "    preprocessor_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    def objective_svc(trial: optuna.trial.Trial):\n",
    "        params = {\n",
    "            'kernel': trial.suggest_categorical('kernel', ['sigmoid','poly','linear','rbf'])\n",
    "        }\n",
    "\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.set_tag('model_family', 'svc')\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param('preprocessor_run_id', preprocessor_run_id)\n",
    "\n",
    "            svc_model = SVC(**params)\n",
    "            svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            y_pred = svc_model.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "\n",
    "            mlflow.log_metric('acc', acc)\n",
    "            mlflow.log_metric('precision', precision)\n",
    "            mlflow.log_metric('f1', f1)\n",
    "            mlflow.log_metric('recall', recall)\n",
    "\n",
    "            signature = infer_signature(X_test_scaled, y_pred)\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                svc_model,\n",
    "                name='svc_model',\n",
    "                input_example=X_test_scaled[:5],\n",
    "                signature=signature\n",
    "            )\n",
    "        \n",
    "        return f1\n",
    "\n",
    "    \n",
    "    sampler = TPESampler(seed=42)\n",
    "    svc_study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    with mlflow.start_run(run_name='Support Vector Classifier (Optuna)', nested=True):\n",
    "        svc_study.optimize(objective_svc, n_trials=2)\n",
    "    \n",
    "    best_params_svc = svc_study.best_params\n",
    "\n",
    "    best_params_svc['random_state'] = 42\n",
    "\n",
    "    return best_params_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af8f19",
   "metadata": {},
   "source": [
    "### <font color= #7fb2b5>• **XGBoost** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e996e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_tuning_xgboost(X_train, X_test, y_train, y_test):\n",
    "    # Habilitar autolog\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    # Preprocess data and log artifacts\n",
    "    X_train_scaled, X_test_scaled, _, encoder, scaler = preprocessor(X_train, X_test, X_val=None, save_artifacts=True)\n",
    "\n",
    "    # Get MLflow ID to store the preprocessing artifacts\n",
    "    preprocessor_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    # Preprocess data and log artifacts\n",
    "    X_train_scaled, X_test_scaled, _, encoder, scaler = preprocessor(X_train, X_test, X_val=None, save_artifacts=True)\n",
    "\n",
    "    # Get MLflow ID to store the preprocessing artifacts\n",
    "    preprocessor_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    # Función objetivo para Optuna\n",
    "    def objective_xgb(trial: optuna.trial.Trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "\n",
    "        with mlflow.start_run(nested=True):\n",
    "            mlflow.set_tag('model_family', 'Xgboost')\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_param('preprocessor_run_id', preprocessor_run_id)\n",
    "\n",
    "            xgb_model = XGBClassifier(**params)\n",
    "            xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            y_pred = xgb_model.predict(X_test_scaled)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "\n",
    "            mlflow.log_metric('acc', acc)\n",
    "            mlflow.log_metric('precision', precision)\n",
    "            mlflow.log_metric('f1', f1)\n",
    "            mlflow.log_metric('recall', recall)\n",
    "\n",
    "            signature = infer_signature(X_test_scaled, y_pred)\n",
    "\n",
    "            mlflow.xgboost.log_model(\n",
    "                xgb_model,\n",
    "                artifact_path='xgboost_model',\n",
    "                input_example=X_test_scaled[:5],\n",
    "                signature=signature\n",
    "            )\n",
    "        \n",
    "        return f1\n",
    "\n",
    "    # Crear y ejecutar el estudio de Optuna\n",
    "    sampler = TPESampler(seed=42)\n",
    "    xgb_study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    with mlflow.start_run(run_name='XGBoost (Optuna)', nested=True):\n",
    "        xgb_study.optimize(objective_xgb, n_trials=3)\n",
    "\n",
    "    # Obtener los mejores parámetros\n",
    "    best_params_xgb = xgb_study.best_params\n",
    "    best_params_xgb['random_state'] = 42\n",
    "\n",
    "    return best_params_xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13b787",
   "metadata": {},
   "source": [
    "## <font color= #bbc28d>• **MLFlow Registry** </font>\n",
    "En esta función se entrenan y evalúan los tres modelos seleccionados: Logistic Regression, SVC y XGBoost, utilizando los mejores hiperparámetros encontrados previamente. Para cada modelo se registran los parámetros, se calculan métricas de desempeño como accuracy, precision, recall y F1-score, y finalmente se almacenan los modelos en MLflow para su seguimiento y futura reutilización. La idea principal es automatizar el entrenamiento, evaluación y registro de los modelos de manera consistente y reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_models(X_train, y_train, X_test, y_test, best_params_lr, best_params_svc, best_params_xgb) -> None:\n",
    "    with mlflow.start_run(run_name='Logistic Regression Model'):\n",
    "        mlflow.log_params(best_params_lr)\n",
    "        mlflow.set_tags({\n",
    "            'project': 'Depression Prediction Project',\n",
    "            'optimizer_engine': 'Optuna',\n",
    "            'model_family': 'logistic_regression',\n",
    "            'feature_set_version': 1,\n",
    "            'candidate': 'true'\n",
    "        })\n",
    "\n",
    "        lr = LogisticRegression(**best_params_lr)\n",
    "        lr.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "        acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "        precision_lr = precision_score(y_test, y_pred_lr)\n",
    "        f1_lr = f1_score(y_test, y_pred_lr)\n",
    "        recall_lr = recall_score(y_test, y_pred_lr)\n",
    "\n",
    "        mlflow.log_metric('acc', acc_lr)\n",
    "        mlflow.log_metric('precision', precision_lr)\n",
    "        mlflow.log_metric('f1', f1_lr)\n",
    "        mlflow.log_metric('recall', recall_lr)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            lr,\n",
    "            name='model'\n",
    "        )\n",
    "    \n",
    "    with mlflow.start_run(run_name='SVC Model'):\n",
    "        mlflow.log_params(best_params_svc)\n",
    "        mlflow.set_tags({\n",
    "            'project': 'Depression Prediction Project',\n",
    "            'optimizer_engine': 'Optuna',\n",
    "            'model_family': 'svc',\n",
    "            'feature_set_version': 1,\n",
    "            'candidate': 'true'\n",
    "        })\n",
    "\n",
    "        svc = SVC(**best_params_svc)\n",
    "        svc.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_svc = svc.predict(X_test)\n",
    "\n",
    "        acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "        precision_svc = precision_score(y_test, y_pred_svc)\n",
    "        f1_svc = f1_score(y_test, y_pred_svc)\n",
    "        recall_svc = recall_score(y_test, y_pred_svc)\n",
    "\n",
    "        mlflow.log_metric('acc', acc_svc)\n",
    "        mlflow.log_metric('precision', precision_svc)\n",
    "        mlflow.log_metric('f1', f1_svc)\n",
    "        mlflow.log_metric('recall', recall_svc)\n",
    "\n",
    "        mlflow.sklearn.log_model(\n",
    "            svc,\n",
    "            name='model'\n",
    "        )\n",
    "    \n",
    "    with mlflow.start_run(run_name='XGBoost Model'):\n",
    "        mlflow.log_params(best_params_xgb)\n",
    "        mlflow.set_tags({\n",
    "            'project': 'Depression Prediction Project',\n",
    "            'optimizer_engine': 'Optuna',\n",
    "            'model_family': 'Trees',\n",
    "            'feature_set_version': 1,\n",
    "            'candidate': 'true'\n",
    "        })\n",
    "\n",
    "        xgb = XGBClassifier(**best_params_xgb)\n",
    "        xgb.fit(X_train, y_train)\n",
    "        y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "        acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "        precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "        f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "        recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "\n",
    "        mlflow.log_metric('acc', acc_xgb)\n",
    "        mlflow.log_metric('precision', precision_xgb)\n",
    "        mlflow.log_metric('f1', f1_xgb)\n",
    "        mlflow.log_metric('recall', recall_xgb)\n",
    "\n",
    "        mlflow.xgboost.log_model(\n",
    "            xgb,\n",
    "            name='model'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfc7df",
   "metadata": {},
   "source": [
    "Esta función se encarga de registrar automáticamente los dos mejores modelos de un experimento en el **Model Registry** de MLflow y asignarles los aliases `Champion` y `Challenger`. \n",
    "1. Primero busca todos los runs marcados como candidatos (`candidate=true`) y los ordena según la métrica F1. \n",
    "2. Luego selecciona los dos primeros: el de mayor F1 se registra como `Champion` y el segundo como `Challenger`. \n",
    "3. Cada modelo se registra en el model registry y se le asigna su alias correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_champion_challenger(experiment_id=\"0\", model_registry_name=\"workspace.\"):\n",
    "    client = MlflowClient()\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=\"tags.candidate = 'true'\",\n",
    "        order_by=[\"metrics.f1 DESC\"]\n",
    "    )\n",
    "\n",
    "    # Tomar los dos primeros runs \n",
    "    champion_run = runs[0]\n",
    "    challenger_run = runs[1] if len(runs) > 1 else None\n",
    "\n",
    "    def register(run, alias):\n",
    "        if run is None:\n",
    "            return\n",
    "        # Registrar modelo\n",
    "        result = mlflow.register_model(\n",
    "            model_uri=f\"runs:/{run.info.run_id}/model\",\n",
    "            name=model_registry_name\n",
    "        )\n",
    "        # Asignar alias\n",
    "        client.set_registered_model_alias(\n",
    "            name=model_registry_name,\n",
    "            alias=alias,\n",
    "            version=result.version\n",
    "        )\n",
    "        print(f\"{alias} registrado: {run.data.tags['model_family']} con F1={run.data.metrics['f1']} (Run ID: {run.info.run_id})\")\n",
    "\n",
    "    # Registrar Champion y Challenger\n",
    "    register(champion_run, \"Champion\")\n",
    "    register(challenger_run, \"Challenger\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depression-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
